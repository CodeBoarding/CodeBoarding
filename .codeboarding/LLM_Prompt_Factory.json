{
  "description": "This system describes a workflow for interacting with Large Language Models (LLMs) for code analysis. The main flow involves a Context Manager preparing relevant information, which is then used by an LLM Prompt Factory to generate tailored prompts. These prompts are passed to an LLM Orchestrator, responsible for managing interactions with various LLM providers and sending the prompts. Finally, a Response Parser processes the raw LLM responses, extracting and structuring the necessary information for further use.",
  "components": [
    {
      "name": "LLM Prompt Factory",
      "description": "Dynamically generates and manages prompts specifically tailored for various Large Language Models (LLMs) and different code analysis tasks. It creates tailored prompts based on specific code analysis tasks, integrates relevant static analysis data and contextual information, adapts prompt structure for different LLM providers, and manages prompt templates.",
      "referenced_source_code": [
        {
          "qualified_name": "agents.prompts.prompt_factory.PromptFactory",
          "reference_file": "agents/prompts/prompt_factory.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ]
    },
    {
      "name": "LLM Orchestrator",
      "description": "Manages the interaction with different LLM providers (e.g., OpenAI, Anthropic, Google Gemini, AWS Bedrock, Ollama), handling API calls, model selection, and response routing.",
      "referenced_source_code": [
        {
          "qualified_name": "LLM Orchestrator",
          "reference_file": "",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ]
    },
    {
      "name": "Response Parser",
      "description": "Extracts and structures relevant information from the raw text responses generated by LLMs. This component is vital for transforming unstructured LLM output into a format usable by subsequent components, such as the Output Generation Engine.",
      "referenced_source_code": [
        {
          "qualified_name": "Response Parser",
          "reference_file": "Response Parser",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ]
    },
    {
      "name": "Context Manager",
      "description": "Prepares and manages the contextual information (e.g., code snippets, static analysis results, architectural patterns) that is fed into the prompts. It aggregates and organizes data from various sources to enrich the LLM's understanding.",
      "referenced_source_code": [
        {
          "qualified_name": "Context Manager",
          "reference_file": "Context Manager",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ]
    },
    {
      "name": "Unclassified",
      "description": "Component for all unclassified files and utility functions (Utility functions/External Libraries/Dependencies)",
      "referenced_source_code": []
    }
  ],
  "components_relations": [
    {
      "relation": "supplies contextual information to",
      "src_name": "Context Manager",
      "dst_name": "LLM Prompt Factory"
    },
    {
      "relation": "provides prepared prompts to",
      "src_name": "LLM Prompt Factory",
      "dst_name": "LLM Orchestrator"
    },
    {
      "relation": "sends LLM responses to",
      "src_name": "LLM Orchestrator",
      "dst_name": "Response Parser"
    },
    {
      "relation": "processes LLM responses from",
      "src_name": "Response Parser",
      "dst_name": "LLM Orchestrator"
    }
  ]
}
