{
  "description": "The Monitoring & Evaluation subsystem captures and persists runtime events, metrics, and structured evaluation results for the CodeBoarding system. Its main flow initializes and configures evaluation runs, instruments LLM and tool pipelines to collect real-time data, executes diverse evaluators (static code analysis and LLM pipeline evaluations), validates and aggregates results according to canonical schemas, and persists artifacts for analysis and reporting. The subsystem thus enables observability, reproducible assessments, and longitudinal tracking of system behavior and evaluation outputs.",
  "components": [
    {
      "name": "Evaluation Orchestration",
      "description": "Primary entry point for the evaluation subsystem: parses CLI args, validates options, initializes environment, and dispatches evaluators to run orchestration flows.",
      "key_entities": [
        {
          "qualified_name": "monitoring.evals.cli:main",
          "reference_file": "evals/cli.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "evals/cli.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Evaluation Framework",
      "description": "Abstract base classes, interfaces, lifecycle management, and shared utilities used by all evaluators to ensure consistent behavior and configuration.",
      "key_entities": [
        {
          "qualified_name": "monitoring.evals.base.EvaluationFramework",
          "reference_file": "evals/base.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.evals.utils",
          "reference_file": "evals/base.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.evals.config",
          "reference_file": "evals/base.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "evals/base.py",
        "evals/config.py",
        "evals/utils.py",
        "logging_config.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Static Analysis Evaluator",
      "description": "Performs static code analysis across the codebase, computes metrics (e.g., LOC, file counts), validates outputs against schemas, and generates static analysis reports.",
      "key_entities": [
        {
          "qualified_name": "monitoring.evals.static_analysis.StaticAnalyzer",
          "reference_file": "evals/definitions/static_analysis.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "evals/definitions/static_analysis.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "LLM Pipeline Evaluator",
      "description": "Evaluates end-to-end LLM and agent pipelines: orchestrates scenarios, captures token/tool usage via monitoring callbacks, aggregates results, and can produce visualizations and scalability measures.",
      "key_entities": [
        {
          "qualified_name": "monitoring.evals.end_to_end.LLMPipelineEvaluator",
          "reference_file": "evals/definitions/end_to_end.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.evals.scalability.ScalabilityEvaluator",
          "reference_file": "evals/definitions/end_to_end.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "evals/definitions/end_to_end.py",
        "evals/definitions/scalability.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Monitoring Core",
      "description": "Real-time collection, aggregation, and contextual scoping of runtime metrics and events; includes callbacks for LLM/tool events, context manager for scoped monitoring, and aggregation/metrics utilities.",
      "key_entities": [
        {
          "qualified_name": "monitoring.callbacks.LLMCallback",
          "reference_file": "monitoring/callbacks.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.context.MonitoringContext",
          "reference_file": "monitoring/context.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.stats.StatsManager",
          "reference_file": "monitoring/stats.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.mixins.MonitoringMixin",
          "reference_file": "monitoring/context.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "monitoring/context.py",
        "monitoring/callbacks.py",
        "monitoring/stats.py",
        "monitoring/__init__.py",
        "monitoring/mixin.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Monitoring Persistence",
      "description": "Durable storage and filesystem organization for collected monitoring outputs and evaluation artifacts, including writers for streaming/incremental writes and utilities for output path management.",
      "key_entities": [
        {
          "qualified_name": "monitoring.writers.MonitoringWriter",
          "reference_file": "monitoring/writers.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.paths",
          "reference_file": "monitoring/paths.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "monitoring/writers.py",
        "monitoring/paths.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Evaluation Data Management",
      "description": "Canonical data contracts (Pydantic models) for inputs, outputs, token breakdowns, and evaluation results to ensure data consistency, validation, and consistent serialization across the subsystem.",
      "key_entities": [
        {
          "qualified_name": "monitoring.evals.schemas",
          "reference_file": "evals/schemas.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "evals/schemas.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    }
  ],
  "components_relations": [
    {
      "relation": "Triggers",
      "src_name": "Evaluation Orchestration",
      "dst_name": "Evaluation Framework"
    },
    {
      "relation": "Configures",
      "src_name": "Evaluation Orchestration",
      "dst_name": "Monitoring Core"
    },
    {
      "relation": "Extends",
      "src_name": "Evaluation Framework",
      "dst_name": "Static Analysis Evaluator"
    },
    {
      "relation": "Extends",
      "src_name": "Evaluation Framework",
      "dst_name": "LLM Pipeline Evaluator"
    },
    {
      "relation": "Utilizes",
      "src_name": "Evaluation Framework",
      "dst_name": "Evaluation Data Management"
    },
    {
      "relation": "Implements",
      "src_name": "Static Analysis Evaluator",
      "dst_name": "Evaluation Framework"
    },
    {
      "relation": "Generates",
      "src_name": "Static Analysis Evaluator",
      "dst_name": "Evaluation Data Management"
    },
    {
      "relation": "Implements",
      "src_name": "LLM Pipeline Evaluator",
      "dst_name": "Evaluation Framework"
    },
    {
      "relation": "Integrates with",
      "src_name": "LLM Pipeline Evaluator",
      "dst_name": "Monitoring Core"
    },
    {
      "relation": "Generates",
      "src_name": "LLM Pipeline Evaluator",
      "dst_name": "Evaluation Data Management"
    },
    {
      "relation": "Collects Data for",
      "src_name": "Monitoring Core",
      "dst_name": "Monitoring Persistence"
    },
    {
      "relation": "Adheres to",
      "src_name": "Monitoring Core",
      "dst_name": "Evaluation Data Management"
    },
    {
      "relation": "Receives Data from",
      "src_name": "Monitoring Persistence",
      "dst_name": "Monitoring Core"
    },
    {
      "relation": "Manages Storage for",
      "src_name": "Monitoring Persistence",
      "dst_name": "Evaluation Data Management"
    },
    {
      "relation": "Defines Structure for",
      "src_name": "Evaluation Data Management",
      "dst_name": "Evaluation Framework"
    },
    {
      "relation": "Defines Structure for",
      "src_name": "Evaluation Data Management",
      "dst_name": "Monitoring Core"
    }
  ]
}