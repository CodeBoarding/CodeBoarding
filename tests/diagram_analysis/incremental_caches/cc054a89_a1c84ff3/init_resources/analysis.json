{
  "description": "The CodeBoarding project is designed as a modular, pipeline-driven system for automated code analysis and documentation generation, heavily leveraging AI. The core data flow begins with the Orchestration & Job Management component, which initiates analysis jobs for a given codebase. This component first triggers the Static Analysis Engine to perform deep code inspection, generating structured data like Control Flow Graphs and code references. Once static analysis is complete, Orchestration & Job Management then dispatches these results to the AI Interpretation Layer. This AI layer, acting as the system's intelligence, utilizes AI Agent Tooling to interact with the codebase and its static analysis data, generating high-level architectural insights, component classifications, and validation reports. Throughout this process, both the AI Interpretation Layer and AI Agent Tooling report their activities and performance metrics to the Monitoring & Evaluation component, ensuring observability and quality assessment. Finally, Orchestration & Job Management consumes the AI-generated insights to produce various documentation outputs, completing the analysis pipeline.",
  "components": [
    {
      "name": "Orchestration & Job Management",
      "description": "This component serves as the central control and coordination hub for the entire CodeBoarding application. It manages job lifecycles, interacts with the job database, handles repository operations, orchestrates the analysis and documentation generation pipeline, and provides the main entry points for both local and CI/CD environments. It is responsible for initiating analysis, managing its state, and generating final documentation outputs.",
      "key_entities": [
        {
          "qualified_name": "main.main",
          "reference_file": "main.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "local_app.LocalApp",
          "reference_file": "local_app.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "github_action.GithubAction",
          "reference_file": "github_action.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "duckdb_crud.DuckDBCrud",
          "reference_file": "duckdb_crud.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "diagram_analysis.diagram_generator.DiagramGenerator",
          "reference_file": "diagram_analysis/diagram_generator.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "logging_config.py",
        "repo_utils/__init__.py",
        "local_app.py",
        "main.py",
        "duckdb_crud.py",
        "github_action.py",
        "vscode_constants.py",
        "utils.py",
        "diagram_analysis/diagram_generator.py",
        "output_generators/markdown.py",
        "diagram_analysis/__init__.py",
        "diagram_analysis/analysis_json.py",
        "diagram_analysis/version.py",
        "output_generators/__init__.py",
        "output_generators/html.py",
        "output_generators/html_template.py",
        "output_generators/mdx.py",
        "output_generators/sphinx.py",
        "repo_utils/errors.py",
        "repo_utils/git_diff.py",
        "repo_utils/ignore.py",
        "setup.py"
      ],
      "source_cluster_ids": [
        5,
        8,
        13
      ],
      "can_expand": true
    },
    {
      "name": "Static Analysis Engine",
      "description": "This component is responsible for performing deep static code analysis across various programming languages. It interacts with Language Server Protocols (LSP) to extract detailed code intelligence, builds Control Flow Graphs (CFGs), manages analysis results, and resolves code references. It provides the foundational, structured code data for further processing by the AI Interpretation Layer.",
      "key_entities": [
        {
          "qualified_name": "static_analyzer.lsp_client.client.LSPClient",
          "reference_file": "static_analyzer/lsp_client/client.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "static_analyzer.graph.CallGraph",
          "reference_file": "static_analyzer/graph.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "static_analyzer.analysis_result.StaticAnalysisResults",
          "reference_file": "static_analyzer/analysis_result.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "static_analyzer.programming_language.ProgrammingLanguage",
          "reference_file": "static_analyzer/programming_language.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "static_analyzer.reference_resolve_mixin.ReferenceResolveMixin",
          "reference_file": "static_analyzer/reference_resolve_mixin.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "static_analyzer/programming_language.py",
        "static_analyzer/lsp_client/client.py",
        "static_analyzer/reference_resolve_mixin.py",
        "static_analyzer/lsp_client/typescript_client.py",
        "static_analyzer/analysis_result.py",
        "static_analyzer/graph.py",
        "static_analyzer/__init__.py",
        "static_analyzer/lsp_client/__init__.py",
        "static_analyzer/scanner.py",
        "static_analyzer/typescript_config_scanner.py"
      ],
      "source_cluster_ids": [
        2,
        6,
        10,
        11,
        12
      ],
      "can_expand": true
    },
    {
      "name": "AI Interpretation Layer",
      "description": "This component is the brain of CodeBoarding, responsible for all AI-driven analysis, interpretation, and validation of code. It leverages various Large Language Models (LLMs) to understand code context, generate insights, and validate architectural elements. It transforms raw or statically analyzed code data into structured architectural insights, component classifications, and validation reports.",
      "key_entities": [
        {
          "qualified_name": "agents.agent.CodeBoardingAgent",
          "reference_file": "agents/agent.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.prompts.prompt_factory.PromptFactory",
          "reference_file": "agents/prompts/prompt_factory.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.agent_responses.AnalysisInsights",
          "reference_file": "agents/agent_responses.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.abstraction_agent.AbstractionAgent",
          "reference_file": "agents/abstraction_agent.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.validator_agent.ValidatorAgent",
          "reference_file": "agents/validator_agent.py",
          "reference_start_line": 20,
          "reference_end_line": 167
        }
      ],
      "assigned_files": [
        "agents/prompts/gemini_flash_prompts_bidirectional.py",
        "agents/abstraction_agent.py",
        "agents/prompts/claude_prompts_unidirectional.py",
        "agents/prompts/gpt_prompts_unidirectional.py",
        "agents/diff_analyzer.py",
        "agents/prompts/abstract_prompt_factory.py",
        "agents/validator_agent.py",
        "agents/agent.py",
        "agents/prompts/gemini_flash_prompts_unidirectional.py",
        "agents/prompts/gpt_prompts_bidirectional.py",
        "agents/prompts/claude_prompts_bidirectional.py",
        "agents/prompts/prompt_factory.py",
        "agents/details_agent.py",
        "agents/agent_responses.py",
        "agents/__init__.py",
        "agents/llm_config.py",
        "agents/meta_agent.py",
        "agents/planner_agent.py",
        "agents/prompts/__init__.py"
      ],
      "source_cluster_ids": [
        1,
        3,
        7,
        9,
        16,
        18,
        19,
        20
      ],
      "can_expand": true
    },
    {
      "name": "AI Agent Tooling",
      "description": "This component provides a comprehensive set of specialized tools that the AI agents (from the AI Interpretation Layer) can invoke to interact with the codebase and its static analysis results. These tools act as a controlled interface, allowing agents to gather specific information without directly accessing the underlying file system or analysis data structures.",
      "key_entities": [
        {
          "qualified_name": "agents.tools.toolkit.CodebaseToolkit",
          "reference_file": "agents/tools/toolkit.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.tools.read_file.ReadFileTool",
          "reference_file": "agents/tools/read_file.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.tools.read_cfg.GetCFGTool",
          "reference_file": "agents/tools/read_cfg.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.tools.read_file_structure.FileStructureTool",
          "reference_file": "agents/tools/read_file_structure.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "agents.tools.read_source.CodeReferenceReader",
          "reference_file": "agents/tools/read_source.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "agents/tools/external_deps.py",
        "agents/tools/read_structure.py",
        "agents/tools/read_source.py",
        "agents/tools/read_docs.py",
        "agents/tools/get_method_invocations.py",
        "agents/tools/read_packages.py",
        "agents/tools/read_cfg.py",
        "agents/tools/read_git_diff.py",
        "agents/tools/read_file_structure.py",
        "agents/tools/toolkit.py",
        "agents/tools/read_file.py",
        "agents/tools/__init__.py",
        "agents/tools/base.py"
      ],
      "source_cluster_ids": [
        4
      ],
      "can_expand": true
    },
    {
      "name": "Monitoring & Evaluation",
      "description": "This component is responsible for observing, tracking, and assessing the performance and quality of the CodeBoarding system. It captures metrics related to LLM usage, tool execution, and provides a framework for running and reporting on various evaluation pipelines. It intercepts events during LLM calls and tool executions to collect usage statistics and performance data.",
      "key_entities": [
        {
          "qualified_name": "monitoring.callbacks.MonitoringCallback",
          "reference_file": "monitoring/callbacks.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.writers.StreamingStatsWriter",
          "reference_file": "monitoring/writers.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "evals.base.BaseEval",
          "reference_file": "evals/base.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "monitoring/callbacks.py",
        "monitoring/writers.py",
        "evals/base.py",
        "evals/cli.py",
        "evals/config.py",
        "evals/definitions/end_to_end.py",
        "evals/definitions/scalability.py",
        "evals/definitions/static_analysis.py",
        "evals/schemas.py",
        "evals/utils.py",
        "monitoring/__init__.py",
        "monitoring/context.py",
        "monitoring/mixin.py",
        "monitoring/paths.py",
        "monitoring/stats.py"
      ],
      "source_cluster_ids": [
        14,
        15,
        17
      ],
      "can_expand": true
    }
  ],
  "components_relations": [
    {
      "relation": "Triggers Static Analysis",
      "src_name": "Orchestration & Job Management",
      "dst_name": "Static Analysis Engine"
    },
    {
      "relation": "Consumes Static Analysis Results",
      "src_name": "Orchestration & Job Management",
      "dst_name": "Static Analysis Engine"
    },
    {
      "relation": "Submits Analysis Jobs",
      "src_name": "Orchestration & Job Management",
      "dst_name": "AI Interpretation Layer"
    },
    {
      "relation": "Consumes Architectural Insights",
      "src_name": "Orchestration & Job Management",
      "dst_name": "AI Interpretation Layer"
    },
    {
      "relation": "Reports Job Status",
      "src_name": "Orchestration & Job Management",
      "dst_name": "Monitoring & Evaluation"
    },
    {
      "relation": "Provides Static Analysis Data",
      "src_name": "Static Analysis Engine",
      "dst_name": "AI Agent Tooling"
    },
    {
      "relation": "Invokes Codebase Tools",
      "src_name": "AI Interpretation Layer",
      "dst_name": "AI Agent Tooling"
    },
    {
      "relation": "Reports LLM Usage Metrics",
      "src_name": "AI Interpretation Layer",
      "dst_name": "Monitoring & Evaluation"
    },
    {
      "relation": "Reports Tool Execution Metrics",
      "src_name": "AI Agent Tooling",
      "dst_name": "Monitoring & Evaluation"
    }
  ]
}