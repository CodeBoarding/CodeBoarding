{
  "description": "This system manages the overall execution flow of various evaluation types, including parsing command-line arguments, setting up the evaluation environment, coordinating the execution of specific evaluation engines, and overseeing the generation of comprehensive reports. It serves as the primary control point for initiating and concluding evaluation runs, while also providing infrastructure for observing runtime events, capturing LLM interactions, and storing all evaluation-related data and artifacts for post-analysis.",
  "components": [
    {
      "name": "Evaluation Orchestrator",
      "description": "Manages the overall execution flow of various evaluation types. This includes parsing command-line arguments, setting up the evaluation environment, coordinating the execution of specific evaluation engines, and overseeing the generation of comprehensive reports. It serves as the primary control point for initiating and concluding evaluation runs.",
      "key_entities": [
        {
          "qualified_name": "evals.cli",
          "reference_file": "evals/cli.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "evals.base.BaseEval",
          "reference_file": "evals/base.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "evals/base.py",
        "evals/cli.py",
        "evals/utils.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Evaluation Engines",
      "description": "Implements the specialized logic for different types of code analysis and performance evaluations, such as end-to-end pipeline analysis, static code analysis for metrics, and scalability analysis. Each engine collects and processes metrics specific to its domain, utilizing project-specific configurations.",
      "key_entities": [
        {
          "qualified_name": "evals.definitions.end_to_end.EndToEndEval",
          "reference_file": "evals/definitions/end_to_end.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "evals.definitions.scalability.ScalabilityEval",
          "reference_file": "evals/definitions/scalability.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "evals.definitions.static_analysis.StaticAnalysisEval",
          "reference_file": "evals/definitions/static_analysis.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "evals.config",
          "reference_file": "evals/config.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "evals/definitions/static_analysis.py",
        "evals/definitions/scalability.py",
        "evals/config.py",
        "evals/definitions/end_to_end.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Event Monitoring & Context",
      "description": "Provides the core infrastructure for observing and capturing runtime events, particularly interactions with Large Language Models (LLMs) and various tools. It offers callback mechanisms to intercept events and establishes execution contexts to enable detailed tracking of operations and resource consumption during evaluation runs.",
      "key_entities": [
        {
          "qualified_name": "monitoring.callbacks.MonitoringCallback",
          "reference_file": "monitoring/callbacks.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.context.MonitorContext",
          "reference_file": "monitoring/context.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.mixin.MonitoringMixin",
          "reference_file": "monitoring/mixin.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "monitoring/callbacks.py",
        "monitoring/context.py",
        "monitoring/mixin.py",
        "monitoring/__init__.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Evaluation Data & Metrics Store",
      "description": "Defines the canonical data structures and schemas for all evaluation-related information, including various metrics, final results, and run metadata, ensuring data consistency across the subsystem. It also aggregates and maintains real-time statistics, suchs as token usage and tool performance, throughout an evaluation's execution.",
      "key_entities": [
        {
          "qualified_name": "evals.schemas",
          "reference_file": "evals/schemas.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.stats.RunStats",
          "reference_file": "monitoring/stats.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "monitoring/stats.py",
        "evals/schemas.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Report & Artifact Persistence",
      "description": "Manages the storage of all evaluation artifacts, raw monitoring data, and generated reports to the file system. It handles the creation of unique run directories, ensures proper organization of output files, and facilitates the retrieval of historical evaluation data for post-analysis.",
      "key_entities": [
        {
          "qualified_name": "monitoring.writers.StreamingStatsWriter",
          "reference_file": "monitoring/writers.py",
          "reference_start_line": null,
          "reference_end_line": null
        },
        {
          "qualified_name": "monitoring.paths",
          "reference_file": "monitoring/paths.py",
          "reference_start_line": null,
          "reference_end_line": null
        }
      ],
      "assigned_files": [
        "monitoring/writers.py",
        "monitoring/paths.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    }
  ],
  "components_relations": [
    {
      "relation": "Initiates",
      "src_name": "Evaluation Orchestrator",
      "dst_name": "Evaluation Engines"
    },
    {
      "relation": "Configures",
      "src_name": "Evaluation Orchestrator",
      "dst_name": "Event Monitoring & Context"
    },
    {
      "relation": "Coordinates Finalization",
      "src_name": "Evaluation Orchestrator",
      "dst_name": "Report & Artifact Persistence"
    },
    {
      "relation": "Informs Completion",
      "src_name": "Evaluation Engines",
      "dst_name": "Evaluation Orchestrator"
    },
    {
      "relation": "Generates Events",
      "src_name": "Evaluation Engines",
      "dst_name": "Event Monitoring & Context"
    },
    {
      "relation": "Stores Data",
      "src_name": "Evaluation Engines",
      "dst_name": "Evaluation Data & Metrics Store"
    },
    {
      "relation": "Captures Events",
      "src_name": "Event Monitoring & Context",
      "dst_name": "Evaluation Engines"
    },
    {
      "relation": "Updates",
      "src_name": "Event Monitoring & Context",
      "dst_name": "Evaluation Data & Metrics Store"
    },
    {
      "relation": "Supplies Data",
      "src_name": "Evaluation Data & Metrics Store",
      "dst_name": "Evaluation Engines"
    },
    {
      "relation": "Provides Data",
      "src_name": "Evaluation Data & Metrics Store",
      "dst_name": "Report & Artifact Persistence"
    },
    {
      "relation": "Consumes Data",
      "src_name": "Report & Artifact Persistence",
      "dst_name": "Evaluation Data & Metrics Store"
    },
    {
      "relation": "Manages Paths With",
      "src_name": "Report & Artifact Persistence",
      "dst_name": "Evaluation Orchestrator"
    }
  ]
}