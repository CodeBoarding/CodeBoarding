{
  "description": "This system provides a robust framework for monitoring the performance and resource usage of the CodeBoarding application, with a particular focus on interactions with Large Language Models (LLMs). It also offers comprehensive capabilities for evaluating the quality of generated analysis, capturing key metrics, and generating detailed reports to assess overall system performance and success.",
  "components": [
    {
      "name": "Evaluation CLI",
      "description": "Serves as the command-line interface for initiating and managing evaluation runs, parsing user arguments, and triggering the evaluation process. It is the primary user-facing entry point for the subsystem.",
      "key_entities": [
        {
          "qualified_name": "main",
          "reference_file": "main.py",
          "reference_start_line": 350,
          "reference_end_line": 459
        }
      ],
      "assigned_files": [
        "evals/cli.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Evaluation Orchestrator",
      "description": "Provides the abstract foundation for all evaluations, offering common functionalities such as report generation, pipeline execution, and managing evaluation results. It acts as the central orchestrator for specific evaluation types.",
      "key_entities": [],
      "assigned_files": [
        "evals/base.py",
        "evals/utils.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Evaluation Configuration Manager",
      "description": "Stores and manages predefined configurations for different types of evaluations (e.g., end-to-end, scaling, static analysis projects), ensuring consistent and repeatable evaluation setups.",
      "key_entities": [],
      "assigned_files": [
        "evals/config.py",
        "evals/definitions/end_to_end.py",
        "evals/definitions/scalability.py",
        "evals/definitions/static_analysis.py",
        "evals/schemas.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Static Analysis Engine",
      "description": "Executes static code analysis, gathering metrics like lines of code (LOC) and file counts, and producing detailed reports on codebase structure. This is a specialized evaluation type.",
      "key_entities": [
        {
          "qualified_name": "StaticAnalyzer",
          "reference_file": "static_analyzer/__init__.py",
          "reference_start_line": 75,
          "reference_end_line": 103
        }
      ],
      "assigned_files": [],
      "source_cluster_ids": [],
      "can_expand": false
    },
    {
      "name": "Pipeline Evaluation Engine",
      "description": "Conducts comprehensive evaluations of the entire project pipeline, aggregating LLM and tool usage data, and generating reports that summarize overall system performance and success. It focuses on end-to-end flow.",
      "key_entities": [
        {
          "qualified_name": "aggregate_llm_usage",
          "reference_file": "evals/definitions/end_to_end.py",
          "reference_start_line": 46,
          "reference_end_line": 74
        }
      ],
      "assigned_files": [],
      "source_cluster_ids": [],
      "can_expand": false
    },
    {
      "name": "Monitoring Data Collector",
      "description": "Intercepts and processes events from LLM and tool executions (e.g., `on_llm_end`, `on_tool_end`), extracting and recording usage statistics like token counts and latency.",
      "key_entities": [
        {
          "qualified_name": "on_llm_end",
          "reference_file": "monitoring/callbacks.py",
          "reference_start_line": 43,
          "reference_end_line": 62
        },
        {
          "qualified_name": "on_tool_end",
          "reference_file": "monitoring/callbacks.py",
          "reference_start_line": 81,
          "reference_end_line": 89
        }
      ],
      "assigned_files": [
        "monitoring/__init__.py",
        "monitoring/callbacks.py",
        "monitoring/context.py",
        "monitoring/mixin.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Runtime Statistics Aggregator",
      "description": "Manages and aggregates runtime statistics for a single evaluation run, including input/output tokens, tool call counts, and error information, providing a consolidated view of performance.",
      "key_entities": [],
      "assigned_files": [
        "monitoring/stats.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    },
    {
      "name": "Reporting & Persistence Module",
      "description": "Persists collected monitoring data, including LLM usage, tool statistics, and run metadata, to designated output files (e.g., JSON, Markdown), and formats them into human-readable reports.",
      "key_entities": [],
      "assigned_files": [
        "monitoring/paths.py",
        "monitoring/writers.py"
      ],
      "source_cluster_ids": [],
      "can_expand": true
    }
  ],
  "components_relations": [
    {
      "relation": "triggers",
      "src_name": "Evaluation CLI",
      "dst_name": "Evaluation Orchestrator"
    },
    {
      "relation": "utilizes for settings",
      "src_name": "Evaluation Orchestrator",
      "dst_name": "Evaluation Configuration Manager"
    },
    {
      "relation": "coordinates",
      "src_name": "Evaluation Orchestrator",
      "dst_name": "Static Analysis Engine"
    },
    {
      "relation": "coordinates",
      "src_name": "Evaluation Orchestrator",
      "dst_name": "Pipeline Evaluation Engine"
    },
    {
      "relation": "integrates with",
      "src_name": "Pipeline Evaluation Engine",
      "dst_name": "Monitoring Data Collector"
    },
    {
      "relation": "forwards data to",
      "src_name": "Monitoring Data Collector",
      "dst_name": "Runtime Statistics Aggregator"
    },
    {
      "relation": "supplies metrics to",
      "src_name": "Runtime Statistics Aggregator",
      "dst_name": "Pipeline Evaluation Engine"
    },
    {
      "relation": "outputs results to",
      "src_name": "Static Analysis Engine",
      "dst_name": "Reporting & Persistence Module"
    },
    {
      "relation": "outputs results to",
      "src_name": "Pipeline Evaluation Engine",
      "dst_name": "Reporting & Persistence Module"
    },
    {
      "relation": "finalizes results with",
      "src_name": "Evaluation Orchestrator",
      "dst_name": "Reporting & Persistence Module"
    }
  ]
}