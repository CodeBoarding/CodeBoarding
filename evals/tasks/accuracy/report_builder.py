from __future__ import annotations

from pathlib import Path

from evals.tasks.accuracy.models import CodeSizeCategory, ScoreHistory
from evals.tasks.accuracy.score_plotter import ScoreHistoryPlotter
from evals.utils import generate_header


class AccuracyReportBuilder:
    """
    Builds markdown reports for accuracy evaluation.

    Uses a fluent builder pattern for composable report construction.

    Example:
        builder = AccuracyReportBuilder(output_dir)
        report = (
            builder
            .with_header(duration_seconds=120.5)
            .with_methodology()
            .with_depth_sections(history, [1, 2])
            .with_score_plot(history)
            .with_reasoning(history)
            .with_glossary()
            .build()
        )
    """

    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self._sections: list[str] = []

    def with_header(self, duration_seconds: float | None = None) -> AccuracyReportBuilder:
        extra_lines = []
        if duration_seconds is not None:
            extra_lines.append(f"**Duration:** {self._format_duration(duration_seconds)}")

        header = generate_header("Accuracy Evaluation", extra_lines=extra_lines)
        self._sections.append(header)
        return self

    def with_methodology(self) -> AccuracyReportBuilder:
        self._sections.append(
            """
This report tracks the structural accuracy of CodeBoarding's generated diagrams against a curated ground-truth dataset. We measure how well the agent captures system topology, component relationships, and architectural hierarchy.

## Methodology & Scoring

Our evaluation uses a **"Model-as-a-Judge"** approach to compare the `analysis.json` generated by CodeBoarding against a human-verified `train.json`.

### Scoring Criteria (1-10)

- **Node Coverage:** Overlap of identified components and nodes.
- **Relationship Fidelity:** Correctness of edges (source, target, and type).
- **Structural Coherence:** Overall topology and flow alignment.

### Glossary

- **Small (S):** < 10k lines of code.
- **Medium (M):** 10k - 100k lines of code.
- **Large (L):** 100k - 1M lines of code.
- **Huge (H):** > 1M lines of code.
"""
        )
        return self

    def with_depth_sections(
        self,
        history: ScoreHistory,
        depth_levels: list[int],
    ) -> AccuracyReportBuilder:
        for depth in depth_levels:
            section = self._build_depth_section(history, depth)
            if section:
                self._sections.append(section)
        return self

    def with_score_plot(
        self,
        history: ScoreHistory,
        depth_levels: list[int] | None = None,
    ) -> AccuracyReportBuilder:
        if len(history.runs) < 1:
            return self

        plotter = ScoreHistoryPlotter(self.output_dir)
        plot_filename = plotter.generate(history, depth_levels)

        if plot_filename:
            self._sections.append(
                f"""
---

### Score History

![Accuracy Score History]({plot_filename})
"""
            )
        return self

    def with_reasoning(self, history: ScoreHistory) -> AccuracyReportBuilder:
        if not history.reasoning:
            return self

        lines = [
            "",
            "### Judge Reasoning",
            "",
            "| Commit | Project | Depth | Score | Node Coverage | Relationship Fidelity | Structural Coherence |",
            "|--------|---------|-------|-------|---------------|----------------------|---------------------|",
        ]

        for r in history.reasoning:
            node_cov = r.node_coverage.replace("|", "\\|")
            rel_fid = r.relationship_fidelity.replace("|", "\\|")
            struct_coh = r.structural_coherence.replace("|", "\\|")
            score_str = f"{r.score:.2f}" if r.score is not None else "N/A"

            lines.append(
                f"| {r.commit} | {r.project} | {r.depth} | {score_str} | {node_cov} | {rel_fid} | {struct_coh} |"
            )

        self._sections.append("\n".join(lines))
        return self

    def with_glossary(self) -> AccuracyReportBuilder:
        self._sections.append(
            """
## Glossary

- **Small (S):** < 10k lines of code.
- **Medium (M):** 10k - 100k lines of code.
- **Large (L):** 100k - 1M lines of code.
- **Huge (H):** > 1M lines of code.
"""
        )
        return self

    def build(self) -> str:
        return "\n".join(self._sections)

    def _build_depth_section(self, history: ScoreHistory, depth: int) -> str | None:
        projects = self._get_projects_for_depth(history, depth)
        if not projects:
            return None

        depth_title = "Architecture Overview" if depth == 1 else "Component Internals"

        lines = [
            "---",
            "",
            f"## Level {depth}: {depth_title}",
            "",
        ]

        lines.extend(self._build_detail_table(history, depth, projects))

        if depth == 2:
            lines.append("")
            lines.append(
                "*Note: To avoid compounding errors from a faulty level 1 generation, "
                "we use our ground truth for level 1, and then generate level 2 from that.*"
            )

        return "\n".join(lines)

    def _get_projects_for_depth(self, history: ScoreHistory, depth: int) -> list[str]:
        suffix = f"-depth-{depth}"
        projects: set[str] = set()

        for run in history.runs:
            for project in run.scores.keys():
                if project.endswith(suffix):
                    projects.add(project)

        return sorted(
            projects,
            key=lambda p: (
                CodeSizeCategory.from_label(history.project_sizes.get(p, "unknown")).order,
                p,
            ),
        )

    def _build_detail_table(
        self,
        history: ScoreHistory,
        depth: int,
        projects: list[str],
    ) -> list[str]:
        header_cols = ["Commit"]
        for project in projects:
            base_name = self._get_base_name(project)
            size_label = history.project_sizes.get(project, "unknown")
            size_char = CodeSizeCategory.from_label(size_label).char
            header_cols.append(f"{base_name} [{size_char}]")
        header_cols.extend(["Avg", "User", "Cores"])

        header = "| " + " | ".join(header_cols) + " |"
        separator = "|" + "|".join(["---"] * len(header_cols)) + "|"

        lines = [
            "### Similarity Scores by Code Size",
            "",
            header,
            separator,
        ]

        for run in history.runs:
            cells = [run.commit]
            scores_for_avg: list[float] = []

            for project in projects:
                score = run.scores.get(project)
                if score is not None:
                    cells.append(f"{score:.2f}")
                    scores_for_avg.append(score)
                else:
                    cells.append("N/A")

            if scores_for_avg:
                cells.append(f"{sum(scores_for_avg) / len(scores_for_avg):.2f}")
            else:
                cells.append("N/A")

            cells.append(run.system_specs.get("User", "N/A"))
            cells.append(run.system_specs.get("Cores", "N/A"))

            lines.append("| " + " | ".join(cells) + " |")

        lines.append("")
        return lines

    def _get_base_name(self, project_name: str) -> str:
        if "-depth-" in project_name:
            return project_name.rsplit("-depth-", 1)[0]
        return project_name

    def _format_duration(self, seconds: float | None) -> str:
        if not isinstance(seconds, (int, float)) or seconds < 0.1:
            return "N/A"
        if seconds >= 3600:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            return f"{hours}h {minutes}m {secs:.1f}s"
        if seconds >= 60:
            minutes = int(seconds // 60)
            secs = seconds % 60
            return f"{minutes}m {secs:.1f}s"
        return f"{seconds:.1f}s"
